{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube video: https://www.youtube.com/watch?v=doDUihpj6ro&t=604s\n",
    "# GitHub Link: https://github.com/nicknochnack/ActionDetectionforSignLanguage/blob/main/Action%20Detection%20Refined.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe keypoints detection and augmentation on the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    # # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) \n",
    "    # # Draw pose/shoulder connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          )\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose/ shoulder connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    #                          ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for all PCs, I placed 1 because of having front and well as back camera in PC\n",
    "from global_variables import CAMERA_NUMBER, WINDOW_NAME, CAMERA_FEED_EXIT_CHAR\n",
    "feed = cv2.VideoCapture(CAMERA_NUMBER)\n",
    "# WINDOW_NAME = 'Webcam feed'\n",
    "# CAMERA_FEED_EXIT_CHAR = 'q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might not run sometimes, theres no bug, it just needs to be run again if the window opens and closes\n",
    "\n",
    "# Set mediapipe model and start the webcam\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic_model:\n",
    "    while feed.isOpened():\n",
    "\n",
    "        # Read image from the feed of the webcam\n",
    "        return_value, frame = feed.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic_model)\n",
    "        # print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        # draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        cv2.imshow(WINDOW_NAME, image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(CAMERA_FEED_EXIT_CHAR):\n",
    "            break\n",
    "\n",
    "    feed.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ability to disable and destroy all camera feeds and Open CV window instances\n",
    "feed.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoints extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # if results.pose_landmarks:\n",
    "    #     pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "    # else:\n",
    "    #     print(\"No pose was detected\")\n",
    "    #     pose = np.zeros(132)\n",
    "\n",
    "    # if results.face_landmarks:\n",
    "    #     face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()\n",
    "    # else:\n",
    "    #     print(\"No face was detected\")\n",
    "    #     face = np.zeros(1404)\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        print(\"Left hand was not detected\")\n",
    "        left_hand = np.zeros(21*3)\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        print(\"Right hand was not detected\")\n",
    "        right_hand = np.zeros(21*3)\n",
    "\n",
    "    # return np.concatenate([pose, face, left_hand, right_hand])\n",
    "    return np.concatenate([left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_keypoints(results)\n",
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving actions that need to be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "import gestures_functions as gf\n",
    "actions = gf.load_gestures()\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder setup for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from global_variables import TRAINING_DATA_FOLDER_NAME, NUMBER_OF_VIDEOS_FOR_EACH_GESTURE, EACH_VIDEO_FRAME_LENGTH\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join(TRAINING_DATA_FOLDER_NAME)\n",
    "\n",
    "# Thirty videos worth of data\n",
    "num_of_videos = NUMBER_OF_VIDEOS_FOR_EACH_GESTURE\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "video_length = EACH_VIDEO_FRAME_LENGTH #frames\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    try:\n",
    "        dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n",
    "    except:\n",
    "        dirmax = 0\n",
    "    for sequence in range(num_of_videos):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = (0, 255, 0)\n",
    "blue = (0, 0, 255)\n",
    "\n",
    "announcement_position = (120, 200)\n",
    "announcement_font_size = 1\n",
    "announcement_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "announcement_color = green\n",
    "annoucement_line_width = 4\n",
    "annoucement_line_type = cv2.LINE_AA\n",
    "\n",
    "text_position = (15, 12)\n",
    "text_font_size = 0.5\n",
    "text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "text_color = blue\n",
    "text_line_width = 1\n",
    "text_line_type = cv2.LINE_AA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main data collection code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_variables import FRAME_COLLECTION_WAIT_TIME, CAMERA_NUMBER\n",
    "\n",
    "feed = cv2.VideoCapture(CAMERA_NUMBER)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through videos\n",
    "        for sequence in range(num_of_videos):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(video_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = feed.read()\n",
    "\n",
    "                if ret != True:\n",
    "                    print(f\"Something is wrong!. Camera feed not accessible\")\n",
    "                    break\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(\n",
    "                        image, \n",
    "                        'STARTING COLLECTION', \n",
    "                        announcement_position, \n",
    "                        announcement_font, \n",
    "                        announcement_font_size, \n",
    "                        announcement_color, \n",
    "                        annoucement_line_width, \n",
    "                        annoucement_line_type\n",
    "                    )\n",
    "                    cv2.putText(\n",
    "                        image, \n",
    "                        f'Collecting frames for \"{action}\" video Number {sequence}', \n",
    "                        text_position, \n",
    "                        text_font, \n",
    "                        text_font_size, \n",
    "                        text_color, \n",
    "                        text_line_width, \n",
    "                        text_line_type\n",
    "                    )\n",
    "                    # Show to screen\n",
    "                    cv2.imshow(WINDOW_NAME, image)\n",
    "                    cv2.waitKey(FRAME_COLLECTION_WAIT_TIME)\n",
    "                else: \n",
    "                    cv2.putText(\n",
    "                        image, \n",
    "                        f'Collecting frames for \"{action}\" video number {sequence}', \n",
    "                        text_position, \n",
    "                        text_font, \n",
    "                        text_font_size, \n",
    "                        text_color, \n",
    "                        text_line_width, \n",
    "                        text_line_type\n",
    "                    )\n",
    "                    # Show to screen\n",
    "                    cv2.imshow(WINDOW_NAME, image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                full_path_to_frame = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(full_path_to_frame, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord(CAMERA_FEED_EXIT_CHAR):\n",
    "                    break\n",
    "                    \n",
    "    feed.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data and create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, labels = [], []\n",
    "for action in actions:\n",
    "    for video in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        video_window = []\n",
    "        for frame_num in range(video_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(video), f\"{frame_num}.npy\"))\n",
    "            video_window.append(res)\n",
    "        videos.append(video_window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(videos).shape  #should be (num_of_video * number of actions, video_length, num of features equal to (the shape of results) all key points identified by mediapipe in each frame)\n",
    "np.array(labels).shape  #should be num_of_video * number of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally getting the X features and their desired Y labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(videos)\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from global_variables import LOGGER_FOLDER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(LOGGER_FOLDER_NAME)\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(actions.shape[1],actions.shape[2])))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_variables import NUM_OF_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard_training_monitor as monitor\n",
    "monitor.launch_tensorboard_monitor()\n",
    "model.fit(X_train, y_train, epochs=NUM_OF_EPOCHS, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_index_of_X_test_you_wanna_manually_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be an array of size equal to number of actions\n",
    "# this array will be hosting values that add up to 1 where the max value means that the model thinks \n",
    "# that this is the most possible solution\n",
    "first_X_test_result = result[the_index_of_X_test_you_wanna_manually_test]\n",
    "first_y_test_result = y_text[the_index_of_X_test_you_wanna_manually_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_of_predicted_action = np.argmax(first_X_test_result)\n",
    "position_of_actual_action = np.argmax(first_y_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_action = actions[position_of_predicted_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_action = actions[position_of_actual_action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = input(\"Enter the name of the model: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{model_name}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
